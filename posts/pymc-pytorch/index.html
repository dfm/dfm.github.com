<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>PyMC3 &#43; PyTorch | Dan Foreman-Mackey</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PyMC3 &#43; PyTorch" />
  <meta name="twitter:description" content=""/>
  <meta name="twitter:site" content="https://twitter.com/exoplaneteer" />
  <meta name="twitter:creator" content="https://twitter.com/exoplaneteer" />
  

  <link rel="shortcut icon" type="image/png" href="../../favicon.png" />


  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="../../css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css" integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="/>
  
    
    <link type="text/css" rel="stylesheet" href="../../css/custom.min.ababf9469c50f82854b25f46557a3ab4d683728134d6c7a55f0169a9446b66cd.css" integrity="sha256-q6v5RpxQ&#43;ChUsl9GVXo6tNaDcoE01selXwFpqURrZs0="/>
  
  
   
   
    

<script type="application/ld+json">
  
    { 
      "@context": "http://schema.org", 
      "@type": "WebSite", 
      "url": "https:\/\/dfm.io\/posts\/pymc-pytorch\/",
      "name": "PyMC3 \u002b PyTorch",
      "author": {
        "@type": "Person",
        "name": ""
      },
      "description": ""
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="../../"> home</a>
      </li>
    
      <li>
        <a  href="../../about">about</a>
      </li>
    
      <li>
        <a  href="../../posts">blog</a>
      </li>
    
      <li>
        <a  href="https://github.com/dfm/cv">cv</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">PyMC3 &#43; PyTorch</h1>
            <time datetime="2019-10-15 00:00:00 &#43;0000 UTC" class="post__date">Oct 15 2019</time> 
            <i>
            
              <p>
                The source for this post can be found <a href="https://github.com/dfm-io/post--pymc-pytorch">here</a>.
                Please open an issue or pull request on that repository if you have questions, comments, or suggestions. 
              </p>
              <p>
                This post is implemented as a Jupyter notebook;
                <a href="https://mybinder.org/v2/gh/dfm-io/post--pymc-pytorch/executed?labpath=post.ipynb">launch an executable version of this post on MyBinder.org</a>.
              </p>
            
            </i>
          </header>
          <article class="post__content">
              
<p>This post is a small extension to <a href="../pymc-tensorflow" 
  
  
>my previous post</a> where I demonstrated that it was possible to combine TensorFlow with PyMC3 to take advantage of the modeling capabilities of TensorFlow while still using the powerful inference engine provided by PyMC3.
The basic procedure involved writing a custom Theano operation that understood how to evaluate a TensorFlow tensor.
In this post, I provide a similar snippet that can be used to combine PyTorch and PyMC3 to a similar end.</p>
<p>One reason why I&rsquo;m interested in these experiments is because I want to use these tools in a fundamentally different way than some other users.
In particular, I spend a lot of my time writing custom ops to extend the modeling languages provided by the standard model building languages since the physically motivated models that I&rsquo;m interested in fitting often aren&rsquo;t easily or efficiently implemented using the existing stack.
For example, I recently release the <a href="http://exoplanet.dfm.io/en/stable/" 
  
   target="_blank" rel="noreferrer noopener" 
>&ldquo;exoplanet&rdquo; library</a> which is an extension to PyMC3 that provides much of the custom functionality needed for fitting astronomical time series data sets.
I chose PyMC3 even though I knew that Theano was deprecated because I found that it had the best combination of powerful inference capabilities and an extensible interface.
With the development of PyMC4, it&rsquo;s not clear that my use case will be well supported since the sampling will be so tightly embedded in TensorFlow.
Furthermore, I don&rsquo;t want to be locked into using TensorFlow just so that I can take advantage of PyMC4&rsquo;s inference algorithms.
Instead, I&rsquo;m interested in understanding how feasible it would be to fork the inference engine part of PyMC3 to build a Python-based inference library that could be used for inference with models defined in TensorFlow-probability, Pyro, Jax, or whatever comes next without a full re-write.
My key thought here is that I don&rsquo;t think that the modeling and inference components of a probabilistic modeling stack need to be as tightly integrated as most existing packages are.</p>
<p>This post doesn&rsquo;t answer the question, but I thought that it was worth sharing anyways.
As in the TensorFlow post, I&rsquo;ll demonstrate this idea using linear regression.
Take a look <a href="../pymc-tensorflow" 
  
  
>at the previous post</a> for more details.
First, let&rsquo;s generate the same simulated data:</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

true_params = np.array([0.5, -2.3, -0.23])

N = 50
t = np.linspace(0, 10, 2)
x = np.random.uniform(0, 10, 50)
y = x * true_params[0] + true_params[1]
y_obs = y + np.exp(true_params[-1]) * np.random.randn(N)

plt.plot(x, y_obs, &quot;.k&quot;, label=&quot;observations&quot;)
plt.plot(t, true_params[0] * t + true_params[1], label=&quot;truth&quot;)
plt.xlabel(&quot;x&quot;)
plt.ylabel(&quot;y&quot;)
plt.legend(fontsize=14);
</code></pre>
<p><img src="output_1_0.png" alt="png"></p>
<p>Next, let&rsquo;s implement the likelihood function for this simple linear model using PyTorch.</p>
<p><strong>Disclaimer:</strong> I&rsquo;m definitely not a PyTorch expert so there might be a better way to do this, but this should at least demonstrate the idea.</p>
<pre><code class="language-python">import torch


class LinearModel(torch.nn.Module):
    def __init__(self):
        super(LinearModel, self).__init__()

        self.m = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))
        self.b = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))
        self.logs = torch.nn.Parameter(torch.tensor(0.0, dtype=torch.float64))

    def forward(self, x, y):
        mean = self.m * x + self.b
        loglike = -0.5 * torch.sum(
            (y - mean) ** 2 * torch.exp(-2 * self.logs) + 2 * self.logs
        )
        return loglike
</code></pre>
<p>Now, here&rsquo;s a custom Theano Op that knows how to evaluate a PyTorch <em>scalar</em> and its gradient.
It would be possible to extend this work for arbitrary PyTorch tensors, but it would take a bit more book keeping.</p>
<pre><code class="language-python">import theano
import theano.tensor as tt


class TorchOp(tt.Op):
    def __init__(self, module, params, args=None):
        self.module = module
        self.params = list(params)
        if args is None:
            self.args = tuple()
        else:
            self.args = tuple(args)

    def make_node(self, *args):
        if len(args) != len(self.params):
            raise ValueError(&quot;dimension mismatch&quot;)
        args = [tt.as_tensor_variable(a) for a in args]
        return theano.graph.basic.Apply(
            self, args, [tt.dscalar().type()] + [a.type() for a in args]
        )

    def infer_shape(self, *args):
        shapes = args[-1]
        return tuple([()] + list(shapes))

    def perform(self, node, inputs, outputs):
        for p, value in zip(self.params, inputs):
            p.data = torch.tensor(value)
            if p.grad is not None:
                p.grad.detach_()
                p.grad.zero_()

        result = self.module(*self.args)
        result.backward()

        outputs[0][0] = result.detach().numpy()
        for i, p in enumerate(self.params):
            outputs[i + 1][0] = p.grad.numpy()

    def grad(self, inputs, gradients):
        for i, g in enumerate(gradients[1:]):
            if not isinstance(g.type, theano.gradient.DisconnectedType):
                raise ValueError(
                    &quot;can't propagate gradients wrt parameter {0}&quot;.format(i + 1)
                )
        return [gradients[0] * d for d in self(*inputs)[1:]]
</code></pre>
<p>Here&rsquo;s how we can combine these into a Theano op that knows how to evaluate the linear model using PyTorch:</p>
<pre><code class="language-python"># Instantiate the PyTorch model
model = torch.jit.script(LinearModel())

# It's useful to select the parameters directly instead of using model.parameters()
# so that we make sure that the order is as expected
params = [model.m, model.b, model.logs]

# The &quot;forward&quot; method of the torch op takes the data as well
args = [torch.tensor(x, dtype=torch.double), torch.tensor(y_obs, dtype=torch.double)]

# Finally put it all together
op = TorchOp(model, params, args=args)
</code></pre>
<p>Now we&rsquo;re all set to use this in a PyMC3 model:</p>
<pre><code class="language-python">import pymc3 as pm

with pm.Model() as torch_model:

    m = pm.Uniform(&quot;m&quot;, -5, 5)
    b = pm.Uniform(&quot;b&quot;, -5, 5)
    logs = pm.Uniform(&quot;logs&quot;, -5, 5)

    pm.Potential(&quot;obs&quot;, op(m, b, logs)[0])

    np.random.seed(6940)
    torch_trace = pm.sample(
        1000, tune=5000, target_accept=0.9, return_inferencedata=True, cores=1
    )
</code></pre>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Sequential sampling (2 chains in 1 job)
NUTS: [logs, b, m]
</code></pre>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [6000/6000 00:22<00:00 Sampling chain 0, 0 divergences]
</div>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [6000/6000 00:21<00:00 Sampling chain 1, 0 divergences]
</div>
<pre><code>Sampling 2 chains for 5_000 tune and 1_000 draw iterations (10_000 + 2_000 draws total) took 44 seconds.
</code></pre>
<p>Note that unlike the TensorFlow implementation, there&rsquo;s no problem using multiple cores with this model and we actually get more than a factor of two increase in efficiency (probably because PyTorch has less Python overhead per call).</p>
<p>For comparison, we can reimplement the same model directly in PyMC3:</p>
<pre><code class="language-python">with pm.Model() as pymc3_model:

    m = pm.Uniform(&quot;m&quot;, -5, 5)
    b = pm.Uniform(&quot;b&quot;, -5, 5)
    logs = pm.Uniform(&quot;logs&quot;, -5, 5)

    pm.Normal(&quot;obs&quot;, mu=m * x + b, sd=pm.math.exp(logs), observed=y_obs)

    np.random.seed(6940)
    pymc3_trace = pm.sample(
        1000, tune=5000, target_accept=0.9, return_inferencedata=True, cores=1
    )
</code></pre>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Sequential sampling (2 chains in 1 job)
NUTS: [logs, b, m]
</code></pre>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [6000/6000 00:06<00:00 Sampling chain 0, 0 divergences]
</div>
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
<div>
  <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [6000/6000 00:06<00:00 Sampling chain 1, 0 divergences]
</div>
<pre><code>Sampling 2 chains for 5_000 tune and 1_000 draw iterations (10_000 + 2_000 draws total) took 13 seconds.
</code></pre>
<p>In this case, the PyMC3 model is about a factor of 2 faster than the PyTorch model, but this is a simple enough model that it&rsquo;s not really a fair comparison.
I expect that this gap would close for more expensive models where the overhead is less important.
Personally, I&rsquo;m willing to pay some performance penalty for the benefit of being able to use whichever modeling framework I want without serious compromises when it comes to inference capabilities.</p>
<p>Finally, we can confirm that we got the same results (they should actually be <em>identical</em> because I used the same random number seed above):</p>
<pre><code class="language-python">import arviz as az

az.summary(torch_trace)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>m</th>
      <td>0.483</td>
      <td>0.037</td>
      <td>0.415</td>
      <td>0.550</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>912.0</td>
      <td>941.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>b</th>
      <td>-2.226</td>
      <td>0.197</td>
      <td>-2.570</td>
      <td>-1.832</td>
      <td>0.007</td>
      <td>0.005</td>
      <td>891.0</td>
      <td>964.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>logs</th>
      <td>-0.299</td>
      <td>0.101</td>
      <td>-0.500</td>
      <td>-0.118</td>
      <td>0.003</td>
      <td>0.002</td>
      <td>1054.0</td>
      <td>1131.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-python">az.summary(pymc3_trace)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>m</th>
      <td>0.483</td>
      <td>0.037</td>
      <td>0.413</td>
      <td>0.548</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>954.0</td>
      <td>1140.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>b</th>
      <td>-2.232</td>
      <td>0.195</td>
      <td>-2.590</td>
      <td>-1.863</td>
      <td>0.006</td>
      <td>0.005</td>
      <td>918.0</td>
      <td>1071.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>logs</th>
      <td>-0.303</td>
      <td>0.102</td>
      <td>-0.493</td>
      <td>-0.110</td>
      <td>0.003</td>
      <td>0.002</td>
      <td>1037.0</td>
      <td>1101.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
<p>Looks right to me!</p>
<p>I think that this experiment seems pretty promising.
It demonstrates that it is feasible to sample a model implemented in PyTorch using PyMC3 with some overhead.
I expect that more expensive models like the ones I normally work on will be even more closely matched in terms of performance.
There are still some open questions, but I think that there&rsquo;s enough here to sketch out a plan for a common Python inference library that could be used with models defined in any modeling framework that can be called from within Python.
To zeroth order, I would like something that can do NUTS sampling where the interface is just a Python function that computes the log-probability and its derivative for a vector of parameters (and maybe some other functions for evaluating deterministics, etc.).
In the long term, it would be awesome to reproduce other functionality of PyMC3 like ADVI, but I have less experience with that so I&rsquo;m not sure exactly what that would take.
Either way, I&rsquo;d love to hear if you have any feedback, tips, questions, pictures of good dogs, etc.</p>


              
                  

<script>
  MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
      displayMath: [
        ["$$", "$$"],
        ["\\[", "\\]"],
      ],
      processEscapes: true,
      processEnvironments: true,
    },
    options: {
      skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
    },
  };

  window.addEventListener("load", (event) => {
    document.querySelectorAll("mjx-container").forEach(function (x) {
      x.parentElement.classList += "has-jax";
    });
  });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  type="text/javascript"
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>

              
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="https://dfm.io/posts/pymc3-mass-matrix/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">Dense mass matrices for PyMC3</span>
    </a>
  

  
    <a class="pagination__item" href="https://dfm.io/posts/extending-jax/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Extending JAX with custom C&#43;&#43; and CUDA code</span>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a
        class="social-icons__link"
        title="GitHub"
        href="https://github.com/dfm"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://dfm.io/svg/github.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="Email"
        href="mailto:dfm@dfm.io"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://dfm.io/svg/email.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="Twitter"
        href="https://twitter.com/exoplaneteer"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://dfm.io/svg/twitter.svg')"></div>
      </a>
    
     
</div>

            <p>© 2014-2022 Dan Foreman-Mackey</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="../../js/index.min.575dda8d49ee02639942c63564273e6da972ab531dda26a08800bdcb477cbd7f.js" integrity="sha256-V13ajUnuAmOZQsY1ZCc&#43;balyq1Md2iagiAC9y0d8vX8=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
